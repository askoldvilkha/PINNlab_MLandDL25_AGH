{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the PINN neural network\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, layers: list, activation: str = 'tanh'):\n",
    "        super(PINN, self).__init__()\n",
    "        self.net = nn.Sequential()\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.net.add_module(f\"layer_{i}\", nn.Linear(layers[i], layers[i+1]))\n",
    "            if i < len(layers) - 2:\n",
    "                if activation.lower() in ['tanh', 'sigmoid', 'relu', 'softplus', 'sin']:\n",
    "                    if activation.lower() == 'tanh':\n",
    "                        self.net.add_module(f\"activation_{i}\", nn.Tanh())\n",
    "                    elif activation.lower() == 'sigmoid':\n",
    "                        self.net.add_module(f\"activation_{i}\", nn.Sigmoid())\n",
    "                    elif activation.lower() == 'relu':\n",
    "                        self.net.add_module(f\"activation_{i}\", nn.ReLU())\n",
    "                    elif activation.lower() == 'softplus':\n",
    "                        self.net.add_module(f\"activation_{i}\", nn.Softplus())\n",
    "                    elif activation.lower() == 'sin':\n",
    "                        class Sin(nn.Module):\n",
    "                            def forward(self, x):\n",
    "                                return torch.sin(x)\n",
    "                        self.net.add_module(f\"activation_{i}\", Sin())\n",
    "                else:\n",
    "                    try:\n",
    "                        self.net.add_module(f\"activation_{i}\", nn.__dict__[activation]())\n",
    "                    except KeyError:\n",
    "                        raise ValueError(f\"Activation function '{activation}' is not recognized.\")\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainParams:\n",
    "    def __init__(self, optimizer: torch.optim.Optimizer, activation: str, ic: list, layers: list):\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        self.x_ic = ic[0]\n",
    "        self.y_ic = ic[1]\n",
    "        self.layers = layers\n",
    "    def write_results(self, epochs: np.ndarray, loss: np.ndarray, time: float):\n",
    "        self.epochs = epochs\n",
    "        self.loss = loss\n",
    "        self.time = time\n",
    "    def write_results_long(self, epochs_arr: np.ndarray, time_arr: np.ndarray):\n",
    "        self.epochs_long = epochs_arr\n",
    "        self.time_long = time_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_residual(x: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n",
    "    # Example: dy/dx + y = 0\n",
    "    y_pred_x = torch.autograd.grad(y_pred, x, grad_outputs=torch.ones_like(y_pred), create_graph=True)[0]\n",
    "    return y_pred_x + y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop function\n",
    "def train(params: TrainParams, ode_residual: callable, length: str = 'flash', save_results: bool = False):\n",
    "    # hyperparameters\n",
    "    model = PINN(params.layers, params.activation)\n",
    "    optimizer = params.optimizer(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # define number of epochs based on length\n",
    "    if length == 'flash':\n",
    "        epochs = 2500\n",
    "    elif length == 'standard':\n",
    "        epochs = 10000\n",
    "    elif length == 'long':\n",
    "        epochs = 30000\n",
    "    else:\n",
    "        raise ValueError(\"Length must be 'flash', 'standard', or 'long'.\")\n",
    "\n",
    "    # initial condition\n",
    "    x_ic = torch.tensor([[params.x_ic]])\n",
    "    y_ic = torch.tensor([[params.y_ic]]) \n",
    "\n",
    "    # training data (collocation points)\n",
    "    x_colloc = torch.linspace(0, 1, 100).view(-1, 1)\n",
    "    x_colloc.requires_grad = True\n",
    "\n",
    "    if save_results:\n",
    "        epochs_record = np.zeros(epochs)\n",
    "        loss_record = np.zeros(epochs)\n",
    "        if length == 'long':\n",
    "            epochs_arr = np.zeros(epochs // 100)\n",
    "            time_arr = np.zeros(epochs // 100)\n",
    "\n",
    "    time_start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_colloc)\n",
    "        res = ode_residual(x_colloc, y_pred)\n",
    "        loss_ode = torch.mean(res**2)\n",
    "\n",
    "        # Initial condition loss\n",
    "        y_ic_pred = model(x_ic)\n",
    "        loss_ic = torch.mean((y_ic_pred - y_ic)**2)\n",
    "\n",
    "        loss = loss_ode + loss_ic\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if save_results:\n",
    "            epochs_record[epoch] = epoch\n",
    "            loss_record[epoch] = loss.item()\n",
    "        \n",
    "        if length == 'long':\n",
    "            if epoch % 100 == 0:\n",
    "                epochs_arr[epoch // 100] = epoch\n",
    "                time_arr[epoch // 100] = time.time() - time_start\n",
    "                \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}, Time elapsed: {time.time() - time_start:.2f}s\")\n",
    "    time_end = time.time()\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(f\"Training completed in {time_elapsed:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.855414092540741, Time elapsed: 0.00s\n",
      "Epoch 100, Loss: 0.6000888347625732, Time elapsed: 0.21s\n",
      "Epoch 200, Loss: 0.5174604654312134, Time elapsed: 0.38s\n",
      "Epoch 300, Loss: 0.4989248514175415, Time elapsed: 0.56s\n",
      "Epoch 400, Loss: 0.48416608572006226, Time elapsed: 0.73s\n",
      "Epoch 500, Loss: 0.4253869950771332, Time elapsed: 0.91s\n",
      "Epoch 600, Loss: 0.2541835308074951, Time elapsed: 1.15s\n",
      "Epoch 700, Loss: 0.11364398896694183, Time elapsed: 1.32s\n",
      "Epoch 800, Loss: 0.07555553317070007, Time elapsed: 1.51s\n",
      "Epoch 900, Loss: 0.054641786962747574, Time elapsed: 1.68s\n",
      "Epoch 1000, Loss: 0.03945804014801979, Time elapsed: 1.85s\n",
      "Epoch 1100, Loss: 0.028378108516335487, Time elapsed: 2.02s\n",
      "Epoch 1200, Loss: 0.02045154944062233, Time elapsed: 2.19s\n",
      "Epoch 1300, Loss: 0.014885226264595985, Time elapsed: 2.35s\n",
      "Epoch 1400, Loss: 0.010996716096997261, Time elapsed: 2.52s\n",
      "Epoch 1500, Loss: 0.008251757360994816, Time elapsed: 2.68s\n",
      "Epoch 1600, Loss: 0.006270250771194696, Time elapsed: 2.85s\n",
      "Epoch 1700, Loss: 0.004796567838639021, Time elapsed: 3.01s\n",
      "Epoch 1800, Loss: 0.0036632786504924297, Time elapsed: 3.18s\n",
      "Epoch 1900, Loss: 0.002764068078249693, Time elapsed: 3.35s\n",
      "Epoch 2000, Loss: 0.002038151491433382, Time elapsed: 3.52s\n",
      "Epoch 2100, Loss: 0.0014586165780201554, Time elapsed: 3.68s\n",
      "Epoch 2200, Loss: 0.0010152377653867006, Time elapsed: 3.85s\n",
      "Epoch 2300, Loss: 0.0006962718907743692, Time elapsed: 4.02s\n",
      "Epoch 2400, Loss: 0.0004813908599317074, Time elapsed: 4.19s\n",
      "Epoch 2500, Loss: 0.00034513475839048624, Time elapsed: 4.35s\n",
      "Epoch 2600, Loss: 0.00026276594144292176, Time elapsed: 4.52s\n",
      "Epoch 2700, Loss: 0.00021407890017144382, Time elapsed: 4.68s\n",
      "Epoch 2800, Loss: 0.00018467052723281085, Time elapsed: 4.85s\n",
      "Epoch 2900, Loss: 0.00016543148376513273, Time elapsed: 5.01s\n",
      "Epoch 3000, Loss: 0.0001511920418124646, Time elapsed: 5.19s\n",
      "Epoch 3100, Loss: 0.00013930830755271018, Time elapsed: 5.36s\n",
      "Epoch 3200, Loss: 0.0001285643083974719, Time elapsed: 5.56s\n",
      "Epoch 3300, Loss: 0.00011846389679703861, Time elapsed: 5.72s\n",
      "Epoch 3400, Loss: 0.00010884435323532671, Time elapsed: 5.90s\n",
      "Epoch 3500, Loss: 9.967554797185585e-05, Time elapsed: 6.06s\n",
      "Epoch 3600, Loss: 9.097367728827521e-05, Time elapsed: 6.23s\n",
      "Epoch 3700, Loss: 8.276303560705855e-05, Time elapsed: 6.43s\n",
      "Epoch 3800, Loss: 7.506238762289286e-05, Time elapsed: 6.60s\n",
      "Epoch 3900, Loss: 6.788317841710523e-05, Time elapsed: 6.77s\n",
      "Epoch 4000, Loss: 6.122771446825936e-05, Time elapsed: 6.94s\n",
      "Epoch 4100, Loss: 5.509299808181822e-05, Time elapsed: 7.10s\n",
      "Epoch 4200, Loss: 4.946910485159606e-05, Time elapsed: 7.27s\n",
      "Epoch 4300, Loss: 4.4340777094475925e-05, Time elapsed: 7.44s\n",
      "Epoch 4400, Loss: 3.9691265556029975e-05, Time elapsed: 7.61s\n",
      "Epoch 4500, Loss: 3.549964458215982e-05, Time elapsed: 7.77s\n",
      "Epoch 4600, Loss: 3.1742605642648414e-05, Time elapsed: 7.94s\n",
      "Epoch 4700, Loss: 2.839624539774377e-05, Time elapsed: 8.10s\n",
      "Epoch 4800, Loss: 2.5433862901991233e-05, Time elapsed: 8.28s\n",
      "Epoch 4900, Loss: 2.28300614253385e-05, Time elapsed: 8.44s\n",
      "Epoch 5000, Loss: 2.0556282834149897e-05, Time elapsed: 8.60s\n",
      "Epoch 5100, Loss: 1.858582800196018e-05, Time elapsed: 8.77s\n",
      "Epoch 5200, Loss: 1.6890824554138817e-05, Time elapsed: 8.94s\n",
      "Epoch 5300, Loss: 1.5444058590219356e-05, Time elapsed: 9.10s\n",
      "Epoch 5400, Loss: 1.4218842807167675e-05, Time elapsed: 9.27s\n",
      "Epoch 5500, Loss: 1.3189067431085277e-05, Time elapsed: 9.44s\n",
      "Epoch 5600, Loss: 1.2330375284363981e-05, Time elapsed: 9.61s\n",
      "Epoch 5700, Loss: 1.1618561075010803e-05, Time elapsed: 9.77s\n",
      "Epoch 5800, Loss: 1.1031823305529542e-05, Time elapsed: 9.93s\n",
      "Epoch 5900, Loss: 1.0550244951446075e-05, Time elapsed: 10.10s\n",
      "Epoch 6000, Loss: 1.015514226310188e-05, Time elapsed: 10.30s\n",
      "Epoch 6100, Loss: 9.830057024373673e-06, Time elapsed: 10.46s\n",
      "Epoch 6200, Loss: 9.560944818076678e-06, Time elapsed: 10.63s\n",
      "Epoch 6300, Loss: 9.335510185337625e-06, Time elapsed: 10.87s\n",
      "Epoch 6400, Loss: 9.143129318545107e-06, Time elapsed: 11.04s\n",
      "Epoch 6500, Loss: 8.97545487532625e-06, Time elapsed: 11.21s\n",
      "Epoch 6600, Loss: 8.825417353364173e-06, Time elapsed: 11.38s\n",
      "Epoch 6700, Loss: 8.687808985996526e-06, Time elapsed: 11.55s\n",
      "Epoch 6800, Loss: 8.558008630643599e-06, Time elapsed: 11.71s\n",
      "Epoch 6900, Loss: 8.433049515588209e-06, Time elapsed: 11.88s\n",
      "Epoch 7000, Loss: 8.310518751386553e-06, Time elapsed: 12.05s\n",
      "Epoch 7100, Loss: 8.188619176507927e-06, Time elapsed: 12.22s\n",
      "Epoch 7200, Loss: 8.066509508353192e-06, Time elapsed: 12.38s\n",
      "Epoch 7300, Loss: 7.94283641880611e-06, Time elapsed: 12.55s\n",
      "Epoch 7400, Loss: 7.8176108218031e-06, Time elapsed: 12.72s\n",
      "Epoch 7500, Loss: 7.691615792282391e-06, Time elapsed: 12.89s\n",
      "Epoch 7600, Loss: 7.565362466266379e-06, Time elapsed: 13.05s\n",
      "Epoch 7700, Loss: 7.472029210475739e-06, Time elapsed: 13.22s\n",
      "Epoch 7800, Loss: 7.311059107451001e-06, Time elapsed: 13.38s\n",
      "Epoch 7900, Loss: 7.183554316725349e-06, Time elapsed: 13.55s\n",
      "Epoch 8000, Loss: 7.0556789069087245e-06, Time elapsed: 13.72s\n",
      "Epoch 8100, Loss: 7.007397471170407e-06, Time elapsed: 13.89s\n",
      "Epoch 8200, Loss: 6.7998348640685435e-06, Time elapsed: 14.05s\n",
      "Epoch 8300, Loss: 6.6720854192681145e-06, Time elapsed: 14.22s\n",
      "Epoch 8400, Loss: 6.5441277001809794e-06, Time elapsed: 14.39s\n",
      "Epoch 8500, Loss: 6.416505129891448e-06, Time elapsed: 14.56s\n",
      "Epoch 8600, Loss: 6.297766049101483e-06, Time elapsed: 14.73s\n",
      "Epoch 8700, Loss: 6.1606983763340395e-06, Time elapsed: 14.90s\n",
      "Epoch 8800, Loss: 6.033727913745679e-06, Time elapsed: 15.06s\n",
      "Epoch 8900, Loss: 5.906410478928592e-06, Time elapsed: 15.23s\n",
      "Epoch 9000, Loss: 5.780312221759232e-06, Time elapsed: 15.39s\n",
      "Epoch 9100, Loss: 5.654452252201736e-06, Time elapsed: 15.56s\n",
      "Epoch 9200, Loss: 5.528337624127744e-06, Time elapsed: 15.73s\n",
      "Epoch 9300, Loss: 5.405149750004057e-06, Time elapsed: 15.90s\n",
      "Epoch 9400, Loss: 5.282076472212793e-06, Time elapsed: 16.06s\n",
      "Epoch 9500, Loss: 5.256980330159422e-06, Time elapsed: 16.23s\n",
      "Epoch 9600, Loss: 5.0361613830318674e-06, Time elapsed: 16.40s\n",
      "Epoch 9700, Loss: 4.915433692076476e-06, Time elapsed: 16.57s\n",
      "Epoch 9800, Loss: 4.797117981070187e-06, Time elapsed: 16.73s\n",
      "Epoch 9900, Loss: 4.6748696149734315e-06, Time elapsed: 16.90s\n",
      "Training completed in 17.07s\n"
     ]
    }
   ],
   "source": [
    "params_test = TrainParams(\n",
    "    optimizer=optim.Adam,\n",
    "    activation='Softmax',\n",
    "    ic=[0.0, 1.0],  # Example: y(0) = 1\n",
    "    layers=[1, 20, 20, 20, 1]\n",
    ")\n",
    "train(params_test, ode_residual, length='standard', save_results=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
